2018-09-18 14:58:43,515   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@53dacd14{/jobs,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,516   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@14d14731{/jobs/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,516   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@2eced48b{/jobs/job,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,516   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@47c4ecdc{/jobs/job/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,517   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@42f33b5d{/stages,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,517   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5c8504fd{/stages/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,518   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@4b7e96a{/stages/stage,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,518   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@6475472c{/stages/stage/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,518   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3f07b12c{/stages/pool,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,518   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@4bd1f8dd{/stages/pool/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,518   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@7096b474{/storage,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,519   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3e14c16d{/storage/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,519   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3c989952{/storage/rdd,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,519   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@784b990c{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,519   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3d3ba765{/environment,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,520   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@25bc0606{/environment/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,520   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5d1659ea{/executors,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,520   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@793138bd{/executors/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,520   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@1951b871{/executors/threadDump,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,521   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5c18016b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,525   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@33aeca0b{/static,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,525   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@43aaf813{/,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,525   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@57ac5227{/api,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,525   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@4ba302e0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,525   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@e98770d{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,536   INFO --- [main]  org.spark_project.jetty.server.ServerConnector(line:266) : Started Spark@45e37a7e{HTTP/1.1}{0.0.0.0:4040}
2018-09-18 14:58:43,537   INFO --- [main]  org.spark_project.jetty.server.Server(line:379) : Started @2227ms
2018-09-18 14:58:43,537   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2018-09-18 14:58:43,540   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.3.199:4040
2018-09-18 14:58:43,610   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2018-09-18 14:58:43,651   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58673.
2018-09-18 14:58:43,652   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.3.199:58673
2018-09-18 14:58:43,653   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-09-18 14:58:43,654   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.3.199, 58673, None)
2018-09-18 14:58:43,657   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.3.199:58673 with 3.0 GB RAM, BlockManagerId(driver, 192.168.3.199, 58673, None)
2018-09-18 14:58:43,660   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.3.199, 58673, None)
2018-09-18 14:58:43,660   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.3.199, 58673, None)
2018-09-18 14:58:43,813   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@687a762c{/metrics/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,831   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2018-09-18 14:58:43,869   INFO --- [main]  org.apache.spark.sql.internal.SharedState(line:54) : Warehouse path is 'file:/C:/Users/wshs/IdeaProjects/logdataSession/spark-warehouse/'.
2018-09-18 14:58:43,874   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@7957dc72{/SQL,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,874   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3aacf32a{/SQL/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,875   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@345e5a17{/SQL/execution,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,875   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@443dbe42{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-09-18 14:58:43,877   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5bbc9f97{/static/sql,null,AVAILABLE,@Spark}
2018-09-18 14:58:46,889   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 214.500035 ms
2018-09-18 14:58:46,955   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: show at logSession.scala:32
2018-09-18 14:58:46,969   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (show at logSession.scala:32) with 1 output partitions
2018-09-18 14:58:46,970   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (show at logSession.scala:32)
2018-09-18 14:58:46,971   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2018-09-18 14:58:46,972   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2018-09-18 14:58:46,976   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at show at logSession.scala:32), which has no missing parents
2018-09-18 14:58:47,081   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 3.0 GB)
2018-09-18 14:58:47,188   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.5 KB, free 3.0 GB)
2018-09-18 14:58:47,193   INFO --- [dispatcher-event-loop-4]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.3.199:58673 (size: 4.5 KB, free: 3.0 GB)
2018-09-18 14:58:47,195   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:996
2018-09-18 14:58:47,199   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at logSession.scala:32)
2018-09-18 14:58:47,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2018-09-18 14:58:47,236   INFO --- [dispatcher-event-loop-5]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5723 bytes)
2018-09-18 14:58:47,243   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2018-09-18 14:58:49,925   INFO --- [Executor task launch worker for task 0]  org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD(line:54) : closed connection
2018-09-18 14:58:49,936   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1582 bytes result sent to driver
2018-09-18 14:58:49,943   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2720 ms on localhost (executor driver) (1/1)
2018-09-18 14:58:49,944   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-09-18 14:58:50,036   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (show at logSession.scala:32) finished in 2.735 s
2018-09-18 14:58:50,041   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: show at logSession.scala:32, took 3.085454 s
2018-09-18 14:58:50,074   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 19.599595 ms
2018-09-18 14:58:50,150   INFO --- [main]  org.apache.spark.sql.execution.SparkSqlParser(line:54) : Parsing command: actionopvidio
2018-09-18 14:58:50,274   INFO --- [main]  org.apache.spark.sql.execution.SparkSqlParser(line:54) : Parsing command: SELECT uid FROM actionopvidio where at = 1
2018-09-18 14:58:50,415   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 7.686298 ms
2018-09-18 14:58:50,423   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: show at logSession.scala:45
2018-09-18 14:58:50,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (show at logSession.scala:45) with 1 output partitions
2018-09-18 14:58:50,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (show at logSession.scala:45)
2018-09-18 14:58:50,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2018-09-18 14:58:50,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2018-09-18 14:58:50,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[6] at show at logSession.scala:45), which has no missing parents
2018-09-18 14:58:50,426   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 7.3 KB, free 3.0 GB)
2018-09-18 14:58:50,430   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KB, free 3.0 GB)
2018-09-18 14:58:50,431   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.3.199:58673 (size: 4.0 KB, free: 3.0 GB)
2018-09-18 14:58:50,431   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:996
2018-09-18 14:58:50,431   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at logSession.scala:45)
2018-09-18 14:58:50,431   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2018-09-18 14:58:50,433   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5723 bytes)
2018-09-18 14:58:50,434   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2018-09-18 14:58:50,585   INFO --- [Executor task launch worker for task 1]  org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD(line:54) : closed connection
2018-09-18 14:58:50,586   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1237 bytes result sent to driver
2018-09-18 14:58:50,587   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 155 ms on localhost (executor driver) (1/1)
2018-09-18 14:58:50,587   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-09-18 14:58:50,587   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (show at logSession.scala:45) finished in 0.155 s
2018-09-18 14:58:50,588   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: show at logSession.scala:45, took 0.164938 s
2018-09-18 14:58:50,594   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 4.937006 ms
2018-09-18 14:58:50,601   INFO --- [main]  org.spark_project.jetty.server.ServerConnector(line:306) : Stopped Spark@45e37a7e{HTTP/1.1}{0.0.0.0:4040}
2018-09-18 14:58:50,603   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@e98770d{/stages/stage/kill,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,603   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@4ba302e0{/jobs/job/kill,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,604   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@57ac5227{/api,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,604   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@43aaf813{/,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,604   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@33aeca0b{/static,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,604   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@5c18016b{/executors/threadDump/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,605   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@1951b871{/executors/threadDump,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,605   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@793138bd{/executors/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,605   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@5d1659ea{/executors,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,605   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@25bc0606{/environment/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,605   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3d3ba765{/environment,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,606   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@784b990c{/storage/rdd/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,606   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3c989952{/storage/rdd,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,606   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3e14c16d{/storage/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,606   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@7096b474{/storage,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@4bd1f8dd{/stages/pool/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3f07b12c{/stages/pool,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@6475472c{/stages/stage/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@4b7e96a{/stages/stage,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,608   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@5c8504fd{/stages/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,608   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@42f33b5d{/stages,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,608   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@47c4ecdc{/jobs/job/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,609   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@2eced48b{/jobs/job,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,609   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@14d14731{/jobs/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,609   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@53dacd14{/jobs,null,UNAVAILABLE,@Spark}
2018-09-18 14:58:50,611   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.3.199:4040
2018-09-18 14:58:50,619   INFO --- [dispatcher-event-loop-6]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2018-09-18 14:58:50,642   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2018-09-18 14:58:50,643   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2018-09-18 14:58:50,648   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2018-09-18 14:58:50,651   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2018-09-18 14:58:50,656   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2018-09-18 14:58:50,658   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2018-09-18 14:58:50,659   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\wshs\AppData\Local\Temp\spark-9a06d516-ff11-4a95-a20f-ef58ede17ea8
2018-09-18 14:59:10,774   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.1.1
2018-09-18 14:59:11,149   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: wshs
2018-09-18 14:59:11,150   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: wshs
2018-09-18 14:59:11,151   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2018-09-18 14:59:11,152   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2018-09-18 14:59:11,153   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(wshs); groups with view permissions: Set(); users  with modify permissions: Set(wshs); groups with modify permissions: Set()
2018-09-18 14:59:11,797   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 58740.
2018-09-18 14:59:11,819   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2018-09-18 14:59:11,837   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2018-09-18 14:59:11,841   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-09-18 14:59:11,842   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2018-09-18 14:59:11,856   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\wshs\AppData\Local\Temp\blockmgr-c60062ab-e873-4329-bad8-3b119e56ea7d
2018-09-18 14:59:11,873   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 3.0 GB
2018-09-18 14:59:11,937   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2018-09-18 14:59:12,015   INFO --- [main]  org.spark_project.jetty.util.log(line:186) : Logging initialized @2206ms
2018-09-18 14:59:12,091   INFO --- [main]  org.spark_project.jetty.server.Server(line:327) : jetty-9.2.z-SNAPSHOT
2018-09-18 14:59:12,105   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@53dacd14{/jobs,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,106   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@14d14731{/jobs/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,106   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@2eced48b{/jobs/job,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,106   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@47c4ecdc{/jobs/job/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,107   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@42f33b5d{/stages,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,107   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5c8504fd{/stages/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,107   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@4b7e96a{/stages/stage,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,108   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@6475472c{/stages/stage/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,108   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3f07b12c{/stages/pool,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,108   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@4bd1f8dd{/stages/pool/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,108   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@7096b474{/storage,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3e14c16d{/storage/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3c989952{/storage/rdd,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@784b990c{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3d3ba765{/environment,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@25bc0606{/environment/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5d1659ea{/executors,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@793138bd{/executors/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@1951b871{/executors/threadDump,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5c18016b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@33aeca0b{/static,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,116   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@43aaf813{/,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,116   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@57ac5227{/api,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,117   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@4ba302e0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,117   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@e98770d{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,129   INFO --- [main]  org.spark_project.jetty.server.ServerConnector(line:266) : Started Spark@45e37a7e{HTTP/1.1}{0.0.0.0:4040}
2018-09-18 14:59:12,130   INFO --- [main]  org.spark_project.jetty.server.Server(line:379) : Started @2322ms
2018-09-18 14:59:12,130   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2018-09-18 14:59:12,133   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.3.199:4040
2018-09-18 14:59:12,209   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2018-09-18 14:59:12,258   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58781.
2018-09-18 14:59:12,258   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.3.199:58781
2018-09-18 14:59:12,260   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-09-18 14:59:12,261   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.3.199, 58781, None)
2018-09-18 14:59:12,264   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.3.199:58781 with 3.0 GB RAM, BlockManagerId(driver, 192.168.3.199, 58781, None)
2018-09-18 14:59:12,267   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.3.199, 58781, None)
2018-09-18 14:59:12,268   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.3.199, 58781, None)
2018-09-18 14:59:12,415   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@687a762c{/metrics/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,435   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2018-09-18 14:59:12,480   INFO --- [main]  org.apache.spark.sql.internal.SharedState(line:54) : Warehouse path is 'file:/C:/Users/wshs/IdeaProjects/logdataSession/spark-warehouse/'.
2018-09-18 14:59:12,488   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@7957dc72{/SQL,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,488   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3aacf32a{/SQL/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,489   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@345e5a17{/SQL/execution,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,490   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@443dbe42{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-09-18 14:59:12,492   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5bbc9f97{/static/sql,null,AVAILABLE,@Spark}
2018-09-18 14:59:15,103   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 218.201813 ms
2018-09-18 14:59:15,169   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: show at logSession.scala:32
2018-09-18 14:59:15,183   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (show at logSession.scala:32) with 1 output partitions
2018-09-18 14:59:15,183   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (show at logSession.scala:32)
2018-09-18 14:59:15,184   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2018-09-18 14:59:15,185   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2018-09-18 14:59:15,189   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at show at logSession.scala:32), which has no missing parents
2018-09-18 14:59:15,296   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 3.0 GB)
2018-09-18 14:59:15,845   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.5 KB, free 3.0 GB)
2018-09-18 14:59:15,849   INFO --- [dispatcher-event-loop-4]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.3.199:58781 (size: 4.5 KB, free: 3.0 GB)
2018-09-18 14:59:15,852   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:996
2018-09-18 14:59:15,855   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at logSession.scala:32)
2018-09-18 14:59:15,857   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2018-09-18 14:59:15,892   INFO --- [dispatcher-event-loop-5]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5723 bytes)
2018-09-18 14:59:15,900   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2018-09-18 14:59:16,966   INFO --- [Executor task launch worker for task 0]  org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD(line:54) : closed connection
2018-09-18 14:59:16,976   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1582 bytes result sent to driver
2018-09-18 14:59:16,983   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1105 ms on localhost (executor driver) (1/1)
2018-09-18 14:59:16,985   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-09-18 14:59:16,987   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (show at logSession.scala:32) finished in 1.122 s
2018-09-18 14:59:16,991   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: show at logSession.scala:32, took 1.822061 s
2018-09-18 14:59:17,029   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 20.790654 ms
2018-09-18 14:59:17,108   INFO --- [main]  org.apache.spark.sql.execution.SparkSqlParser(line:54) : Parsing command: actionopvidio
2018-09-18 14:59:17,225   INFO --- [main]  org.apache.spark.sql.execution.SparkSqlParser(line:54) : Parsing command: SELECT uid,at FROM actionopvidio where at = 1
2018-09-18 14:59:17,359   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 8.7124 ms
2018-09-18 14:59:17,367   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: show at logSession.scala:45
2018-09-18 14:59:17,368   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (show at logSession.scala:45) with 1 output partitions
2018-09-18 14:59:17,368   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (show at logSession.scala:45)
2018-09-18 14:59:17,368   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2018-09-18 14:59:17,368   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2018-09-18 14:59:17,369   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[6] at show at logSession.scala:45), which has no missing parents
2018-09-18 14:59:17,371   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 7.2 KB, free 3.0 GB)
2018-09-18 14:59:17,375   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.1 KB, free 3.0 GB)
2018-09-18 14:59:17,376   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.3.199:58781 (size: 4.1 KB, free: 3.0 GB)
2018-09-18 14:59:17,377   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:996
2018-09-18 14:59:17,377   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at logSession.scala:45)
2018-09-18 14:59:17,378   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2018-09-18 14:59:17,380   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5723 bytes)
2018-09-18 14:59:17,380   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2018-09-18 14:59:17,783   INFO --- [Executor task launch worker for task 1]  org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD(line:54) : closed connection
2018-09-18 14:59:17,784   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1323 bytes result sent to driver
2018-09-18 14:59:17,785   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 407 ms on localhost (executor driver) (1/1)
2018-09-18 14:59:17,786   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-09-18 14:59:17,786   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (show at logSession.scala:45) finished in 0.408 s
2018-09-18 14:59:17,787   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: show at logSession.scala:45, took 0.419434 s
2018-09-18 14:59:17,796   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 8.337415 ms
2018-09-18 14:59:17,803   INFO --- [main]  org.spark_project.jetty.server.ServerConnector(line:306) : Stopped Spark@45e37a7e{HTTP/1.1}{0.0.0.0:4040}
2018-09-18 14:59:17,805   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@e98770d{/stages/stage/kill,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,805   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@4ba302e0{/jobs/job/kill,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,806   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@57ac5227{/api,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,806   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@43aaf813{/,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,806   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@33aeca0b{/static,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,806   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@5c18016b{/executors/threadDump/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,806   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@1951b871{/executors/threadDump,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,806   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@793138bd{/executors/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@5d1659ea{/executors,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@25bc0606{/environment/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3d3ba765{/environment,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@784b990c{/storage/rdd/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3c989952{/storage/rdd,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3e14c16d{/storage/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@7096b474{/storage,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@4bd1f8dd{/stages/pool/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3f07b12c{/stages/pool,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@6475472c{/stages/stage/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@4b7e96a{/stages/stage,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@5c8504fd{/stages/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,809   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@42f33b5d{/stages,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,809   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@47c4ecdc{/jobs/job/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,809   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@2eced48b{/jobs/job,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,809   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@14d14731{/jobs/json,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,809   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@53dacd14{/jobs,null,UNAVAILABLE,@Spark}
2018-09-18 14:59:17,811   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.3.199:4040
2018-09-18 14:59:17,824   INFO --- [dispatcher-event-loop-6]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2018-09-18 14:59:17,848   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2018-09-18 14:59:17,850   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2018-09-18 14:59:17,855   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2018-09-18 14:59:17,858   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2018-09-18 14:59:17,865   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2018-09-18 14:59:17,868   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2018-09-18 14:59:17,870   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\wshs\AppData\Local\Temp\spark-572db53a-6573-40b2-bef7-31f5339d129e
2018-09-18 15:00:17,293   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.1.1
2018-09-18 15:00:17,678   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: wshs
2018-09-18 15:00:17,679   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: wshs
2018-09-18 15:00:17,679   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2018-09-18 15:00:17,680   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2018-09-18 15:00:17,680   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(wshs); groups with view permissions: Set(); users  with modify permissions: Set(wshs); groups with modify permissions: Set()
2018-09-18 15:00:18,211   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 58878.
2018-09-18 15:00:18,230   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2018-09-18 15:00:18,250   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2018-09-18 15:00:18,253   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-09-18 15:00:18,253   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2018-09-18 15:00:18,270   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\wshs\AppData\Local\Temp\blockmgr-4d06fa70-712c-4fd4-9e16-88baa3a3ea41
2018-09-18 15:00:18,289   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 3.0 GB
2018-09-18 15:00:18,358   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2018-09-18 15:00:18,441   INFO --- [main]  org.spark_project.jetty.util.log(line:186) : Logging initialized @2056ms
2018-09-18 15:00:18,519   INFO --- [main]  org.spark_project.jetty.server.Server(line:327) : jetty-9.2.z-SNAPSHOT
2018-09-18 15:00:18,531   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@53dacd14{/jobs,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,532   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@14d14731{/jobs/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,532   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@2eced48b{/jobs/job,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,532   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@47c4ecdc{/jobs/job/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,533   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@42f33b5d{/stages,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,533   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5c8504fd{/stages/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,533   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@4b7e96a{/stages/stage,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,533   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@6475472c{/stages/stage/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,533   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3f07b12c{/stages/pool,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,534   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@4bd1f8dd{/stages/pool/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,534   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@7096b474{/storage,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,534   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3e14c16d{/storage/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,534   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3c989952{/storage/rdd,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,535   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@784b990c{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,535   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3d3ba765{/environment,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,535   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@25bc0606{/environment/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,535   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5d1659ea{/executors,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,535   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@793138bd{/executors/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,536   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@1951b871{/executors/threadDump,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,536   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5c18016b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,540   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@33aeca0b{/static,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,540   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@43aaf813{/,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,541   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@57ac5227{/api,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,541   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@4ba302e0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,541   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@e98770d{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,554   INFO --- [main]  org.spark_project.jetty.server.ServerConnector(line:266) : Started Spark@45e37a7e{HTTP/1.1}{0.0.0.0:4040}
2018-09-18 15:00:18,554   INFO --- [main]  org.spark_project.jetty.server.Server(line:379) : Started @2171ms
2018-09-18 15:00:18,555   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2018-09-18 15:00:18,557   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.3.199:4040
2018-09-18 15:00:18,623   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2018-09-18 15:00:18,665   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58919.
2018-09-18 15:00:18,666   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.3.199:58919
2018-09-18 15:00:18,668   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-09-18 15:00:18,669   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.3.199, 58919, None)
2018-09-18 15:00:18,672   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.3.199:58919 with 3.0 GB RAM, BlockManagerId(driver, 192.168.3.199, 58919, None)
2018-09-18 15:00:18,675   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.3.199, 58919, None)
2018-09-18 15:00:18,676   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.3.199, 58919, None)
2018-09-18 15:00:18,825   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@687a762c{/metrics/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,842   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2018-09-18 15:00:18,877   INFO --- [main]  org.apache.spark.sql.internal.SharedState(line:54) : Warehouse path is 'file:/C:/Users/wshs/IdeaProjects/logdataSession/spark-warehouse/'.
2018-09-18 15:00:18,881   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@7957dc72{/SQL,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,882   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@3aacf32a{/SQL/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,883   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@345e5a17{/SQL/execution,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,884   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@443dbe42{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-09-18 15:00:18,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:744) : Started o.s.j.s.ServletContextHandler@5bbc9f97{/static/sql,null,AVAILABLE,@Spark}
2018-09-18 15:00:21,430   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 213.488956 ms
2018-09-18 15:00:21,492   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: show at logSession.scala:32
2018-09-18 15:00:21,506   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (show at logSession.scala:32) with 1 output partitions
2018-09-18 15:00:21,507   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (show at logSession.scala:32)
2018-09-18 15:00:21,507   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2018-09-18 15:00:21,508   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2018-09-18 15:00:21,514   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at show at logSession.scala:32), which has no missing parents
2018-09-18 15:00:21,614   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 3.0 GB)
2018-09-18 15:00:21,719   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.5 KB, free 3.0 GB)
2018-09-18 15:00:21,723   INFO --- [dispatcher-event-loop-4]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.3.199:58919 (size: 4.5 KB, free: 3.0 GB)
2018-09-18 15:00:21,726   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:996
2018-09-18 15:00:21,730   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at logSession.scala:32)
2018-09-18 15:00:21,731   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2018-09-18 15:00:21,768   INFO --- [dispatcher-event-loop-5]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5723 bytes)
2018-09-18 15:00:21,777   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2018-09-18 15:00:22,961   INFO --- [Executor task launch worker for task 0]  org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD(line:54) : closed connection
2018-09-18 15:00:22,973   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1582 bytes result sent to driver
2018-09-18 15:00:22,981   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1227 ms on localhost (executor driver) (1/1)
2018-09-18 15:00:22,983   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-09-18 15:00:22,985   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (show at logSession.scala:32) finished in 1.242 s
2018-09-18 15:00:22,990   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: show at logSession.scala:32, took 1.497476 s
2018-09-18 15:00:23,032   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 22.446539 ms
2018-09-18 15:00:23,108   INFO --- [main]  org.apache.spark.sql.execution.SparkSqlParser(line:54) : Parsing command: actionopvidio
2018-09-18 15:00:23,279   INFO --- [main]  org.apache.spark.sql.execution.SparkSqlParser(line:54) : Parsing command: SELECT uid,at,ct FROM actionopvidio where at = 1
2018-09-18 15:00:23,478   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 11.286218 ms
2018-09-18 15:00:23,500   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: show at logSession.scala:45
2018-09-18 15:00:23,503   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (show at logSession.scala:45) with 1 output partitions
2018-09-18 15:00:23,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (show at logSession.scala:45)
2018-09-18 15:00:23,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2018-09-18 15:00:23,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2018-09-18 15:00:23,506   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[6] at show at logSession.scala:45), which has no missing parents
2018-09-18 15:00:23,510   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 7.5 KB, free 3.0 GB)
2018-09-18 15:00:23,514   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KB, free 3.0 GB)
2018-09-18 15:00:23,515   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.3.199:58919 (size: 4.2 KB, free: 3.0 GB)
2018-09-18 15:00:23,515   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:996
2018-09-18 15:00:23,516   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at logSession.scala:45)
2018-09-18 15:00:23,516   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2018-09-18 15:00:23,517   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5723 bytes)
2018-09-18 15:00:23,518   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2018-09-18 15:00:23,766   INFO --- [Executor task launch worker for task 1]  org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD(line:54) : closed connection
2018-09-18 15:00:23,767   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1390 bytes result sent to driver
2018-09-18 15:00:23,769   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 253 ms on localhost (executor driver) (1/1)
2018-09-18 15:00:23,769   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-09-18 15:00:23,769   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (show at logSession.scala:45) finished in 0.253 s
2018-09-18 15:00:23,769   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: show at logSession.scala:45, took 0.269245 s
2018-09-18 15:00:23,782   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 11.609823 ms
2018-09-18 15:00:23,789   INFO --- [main]  org.spark_project.jetty.server.ServerConnector(line:306) : Stopped Spark@45e37a7e{HTTP/1.1}{0.0.0.0:4040}
2018-09-18 15:00:23,791   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@e98770d{/stages/stage/kill,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,791   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@4ba302e0{/jobs/job/kill,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,791   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@57ac5227{/api,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,791   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@43aaf813{/,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,791   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@33aeca0b{/static,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@5c18016b{/executors/threadDump/json,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@1951b871{/executors/threadDump,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@793138bd{/executors/json,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@5d1659ea{/executors,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@25bc0606{/environment/json,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3d3ba765{/environment,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@784b990c{/storage/rdd/json,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,793   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3c989952{/storage/rdd,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,793   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3e14c16d{/storage/json,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,793   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@7096b474{/storage,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,793   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@4bd1f8dd{/stages/pool/json,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,794   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@3f07b12c{/stages/pool,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,794   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@6475472c{/stages/stage/json,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,794   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@4b7e96a{/stages/stage,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,794   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@5c8504fd{/stages/json,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,794   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@42f33b5d{/stages,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,795   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@47c4ecdc{/jobs/job/json,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,795   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@2eced48b{/jobs/job,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,795   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@14d14731{/jobs/json,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,795   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:865) : Stopped o.s.j.s.ServletContextHandler@53dacd14{/jobs,null,UNAVAILABLE,@Spark}
2018-09-18 15:00:23,797   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.3.199:4040
2018-09-18 15:00:23,807   INFO --- [dispatcher-event-loop-6]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2018-09-18 15:00:23,831   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2018-09-18 15:00:23,831   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2018-09-18 15:00:23,836   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2018-09-18 15:00:23,839   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2018-09-18 15:00:23,845   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2018-09-18 15:00:23,847   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2018-09-18 15:00:23,848   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\wshs\AppData\Local\Temp\spark-fe53943f-ea72-4c74-ad6b-c55be217b55b
2018-09-21 15:00:02,727  ERROR --- [Executor task launch worker for task 77]  org.apache.spark.executor.Executor(line:91) : Exception in task 2.0 in stage 1.0 (TID 77)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,728  ERROR --- [Executor task launch worker for task 83]  org.apache.spark.executor.Executor(line:91) : Exception in task 13.0 in stage 1.0 (TID 83)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,727  ERROR --- [Executor task launch worker for task 80]  org.apache.spark.executor.Executor(line:91) : Exception in task 7.0 in stage 1.0 (TID 80)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,727  ERROR --- [Executor task launch worker for task 79]  org.apache.spark.executor.Executor(line:91) : Exception in task 6.0 in stage 1.0 (TID 79)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,727  ERROR --- [Executor task launch worker for task 78]  org.apache.spark.executor.Executor(line:91) : Exception in task 5.0 in stage 1.0 (TID 78)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,727  ERROR --- [Executor task launch worker for task 82]  org.apache.spark.executor.Executor(line:91) : Exception in task 10.0 in stage 1.0 (TID 82)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,727  ERROR --- [Executor task launch worker for task 76]  org.apache.spark.executor.Executor(line:91) : Exception in task 1.0 in stage 1.0 (TID 76)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,727  ERROR --- [Executor task launch worker for task 81]  org.apache.spark.executor.Executor(line:91) : Exception in task 8.0 in stage 1.0 (TID 81)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,784  ERROR --- [Executor task launch worker for task 86]  org.apache.spark.executor.Executor(line:91) : Exception in task 20.0 in stage 1.0 (TID 86)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,786  ERROR --- [Executor task launch worker for task 88]  org.apache.spark.executor.Executor(line:91) : Exception in task 24.0 in stage 1.0 (TID 88)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,788  ERROR --- [Executor task launch worker for task 89]  org.apache.spark.executor.Executor(line:91) : Exception in task 26.0 in stage 1.0 (TID 89)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,789  ERROR --- [Executor task launch worker for task 90]  org.apache.spark.executor.Executor(line:91) : Exception in task 28.0 in stage 1.0 (TID 90)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,791  ERROR --- [Executor task launch worker for task 84]  org.apache.spark.executor.Executor(line:91) : Exception in task 14.0 in stage 1.0 (TID 84)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,791  ERROR --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:70) : Task 8 in stage 1.0 failed 1 times; aborting job
2018-09-21 15:00:02,793  ERROR --- [Executor task launch worker for task 87]  org.apache.spark.executor.Executor(line:91) : Exception in task 22.0 in stage 1.0 (TID 87)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,796  ERROR --- [Executor task launch worker for task 91]  org.apache.spark.executor.Executor(line:91) : Exception in task 30.0 in stage 1.0 (TID 91)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:00:02,797  ERROR --- [Executor task launch worker for task 85]  org.apache.spark.executor.Executor(line:91) : Exception in task 16.0 in stage 1.0 (TID 85)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:60)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:02:28,482  ERROR --- [Executor task launch worker for task 81]  org.apache.spark.executor.Executor(line:91) : Exception in task 8.0 in stage 1.0 (TID 81)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:62)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:02:28,485  ERROR --- [Executor task launch worker for task 76]  org.apache.spark.executor.Executor(line:91) : Exception in task 1.0 in stage 1.0 (TID 76)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:62)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:02:28,485  ERROR --- [Executor task launch worker for task 79]  org.apache.spark.executor.Executor(line:91) : Exception in task 6.0 in stage 1.0 (TID 79)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:62)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:02:28,484  ERROR --- [Executor task launch worker for task 80]  org.apache.spark.executor.Executor(line:91) : Exception in task 7.0 in stage 1.0 (TID 80)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:62)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:02:28,484  ERROR --- [Executor task launch worker for task 77]  org.apache.spark.executor.Executor(line:91) : Exception in task 2.0 in stage 1.0 (TID 77)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:62)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:02:28,483  ERROR --- [Executor task launch worker for task 78]  org.apache.spark.executor.Executor(line:91) : Exception in task 5.0 in stage 1.0 (TID 78)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:62)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:02:28,482  ERROR --- [Executor task launch worker for task 82]  org.apache.spark.executor.Executor(line:91) : Exception in task 10.0 in stage 1.0 (TID 82)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:62)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:02:28,494  ERROR --- [Executor task launch worker for task 83]  org.apache.spark.executor.Executor(line:91) : Exception in task 13.0 in stage 1.0 (TID 83)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:62)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:02:28,505  ERROR --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:70) : Task 13 in stage 1.0 failed 1 times; aborting job
2018-09-21 15:02:28,511  ERROR --- [Executor task launch worker for task 85]  org.apache.spark.executor.Executor(line:91) : Exception in task 16.0 in stage 1.0 (TID 85)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:62)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:02:28,511  ERROR --- [Executor task launch worker for task 84]  org.apache.spark.executor.Executor(line:91) : Exception in task 14.0 in stage 1.0 (TID 84)
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.runtime.Nothing$
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:62)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:08:12,817  ERROR --- [Executor task launch worker for task 79]  org.apache.spark.executor.Executor(line:91) : Exception in task 6.0 in stage 1.0 (TID 79)
java.lang.ArrayIndexOutOfBoundsException: 2
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:71)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:08:12,821  ERROR --- [Executor task launch worker for task 76]  org.apache.spark.executor.Executor(line:91) : Exception in task 1.0 in stage 1.0 (TID 76)
java.lang.ArrayIndexOutOfBoundsException: 2
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:71)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:08:12,821  ERROR --- [Executor task launch worker for task 77]  org.apache.spark.executor.Executor(line:91) : Exception in task 2.0 in stage 1.0 (TID 77)
java.lang.ArrayIndexOutOfBoundsException: 2
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:71)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:08:12,819  ERROR --- [Executor task launch worker for task 82]  org.apache.spark.executor.Executor(line:91) : Exception in task 10.0 in stage 1.0 (TID 82)
java.lang.ArrayIndexOutOfBoundsException: 2
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:71)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:08:12,819  ERROR --- [Executor task launch worker for task 78]  org.apache.spark.executor.Executor(line:91) : Exception in task 5.0 in stage 1.0 (TID 78)
java.lang.ArrayIndexOutOfBoundsException: 2
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:71)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:08:12,817  ERROR --- [Executor task launch worker for task 81]  org.apache.spark.executor.Executor(line:91) : Exception in task 8.0 in stage 1.0 (TID 81)
java.lang.ArrayIndexOutOfBoundsException: 2
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:71)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:08:12,826  ERROR --- [Executor task launch worker for task 80]  org.apache.spark.executor.Executor(line:91) : Exception in task 7.0 in stage 1.0 (TID 80)
java.lang.ArrayIndexOutOfBoundsException: 2
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:71)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:08:12,825  ERROR --- [Executor task launch worker for task 83]  org.apache.spark.executor.Executor(line:91) : Exception in task 13.0 in stage 1.0 (TID 83)
java.lang.ArrayIndexOutOfBoundsException: 2
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:71)
	at logSession$$anonfun$getData_uid_rid_cidRDD$1$1.apply(logSession.scala:61)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-09-21 15:08:12,842  ERROR --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:70) : Task 5 in stage 1.0 failed 1 times; aborting job
